= Grand Central Dispatch for MacRuby

== Introduction

This article explains how to use Grand Central Dispatch (*GCD*) from MacRuby, and is adapted from {Introducing Blocks and Grand Central Dispatch}[http://developer.apple.com/mac/articles/cocoa/introblocksgcd.html] at the {Apple Developer Connection}[http://developer.apple.com/].

=== About GCD
	
GCD is a revolutionary approach to multicore computing that is woven throughout the fabric of {Mac OS X}[http://www.apple.com/macosx/] version 10.6 Snow Leopard. GCD combines an easy-to-use programming model with highly-efficient system services to radically simplify the code needed to make best use of multiple processors. The technologies in GCD improve the performance, efficiency, and responsiveness of Snow Leopard out of the box, and will deliver even greater benefits as more developers adopt them.

The central insight of GCD is shifting the responsibility for managing threads and their execution from applications to the operating system. As a result, programmers can write less code to deal with concurrent operations in their applications, and the system can perform more efficiently on single-processor machines, large multiprocessor servers, and everything in between. Without a pervasive approach such as GCD, even the best-written application cannot deliver optimal performance, because it doesn’t have full insight into everything else happening in the system. 

=== The MacRuby Dispatch module

GCD is natively implemented as a C API and runtime engine.  MacRuby 0.5 introduces a new "Dispatch" module, which provides a Ruby wrapper around that API. This allows Ruby blocks to be scheduled on queues for asynchronous and concurrent execution either explicitly or in response to various kinds of events, with GCD automatically mapping queues to threads as needed.  The Dispatch module provides four primary abstractions that mirror the C API:

Dispatch::Queue:: The basic unit for organizing blocks. Several queues are created by default, and applications may create additional queues for their own use.

Dispatch::Group:: Allows applications to track the progress of blocks submitted to queues and take action when the blocks complete.

Dispatch::Source:: Monitors and coalesces low-level system events so that they can be responded to asynchronously via simple event handlers.

Dispatch::Semaphore:: Synchronizes threads via a combination of waiting and signalling.

In addition, MacRuby 0.6 provides additional, higher-level abstractions and convenience APIs via the "dispatch" library (i.e., +require 'dispatch'+).

=== What You Need

As the MacRuby 0.6 features help reduce the learning curve for GCD, we will assume those for the remainder of this article.  Note that MacRuby 0.6 is currently (as of Feb 2010) only available via the source[http://www.macruby.org/source.html] or the {nightly builds}[http://www.icoretech.org/2009/09/macruby-nightlies/].

We also assume that you are already familiar with Ruby, though not necessarily MacRuby. No prior knowledge of C or GCD is assumed or required, but the {dispatch(3) man page}[http://developer.apple.com/mac/library/DOCUMENTATION/Darwin/Reference/ManPages/man3/dispatch.3.html] may be helpful if you wish to better understand the underlying semantics.

== Dispatch.fork

The most basic method of the Dispatch module is +fork+, which allows you to schedule work asynchronously.

	require 'dispatch'
	Dispatch.fork { p "Do this somewhere else" }

This atomically[http://en.wikipedia.org/wiki/Atomic_operation] adds each block to GCD's default concurrent queue, then returns immediately. Concurrent queues schedule as many simultaneous blocks as they can on a first-in/first-out (FIFO[http://en.wikipedia.org/wiki/FIFO]) basis, as long as there are threads available.  If there are spare CPUs, the system will automatically create more threads -- and reclaim them when idle -- allowing GCD to dynamically scale the number of threads based on the overall system load.

=== Dispatch::Job

Invoking +fork+ returns a +Job+ object which can be used to track completion of work:

	job = Dispatch.fork { @sum = 2 + 2 }
	
You can pass that job to multiple forks, to track them as a group:

	Dispatch.fork(job) { @product = 3 * 3 }

Call +join+ to wait until all the work completes:

	job.join
	puts @sum + @product # => 10
	
Call +value+ to retrieve the return value of block passed in when the job was created:
	
	puts job.value # => 4
	
This will first call +join+ to ensure the value has been calculated, allowing it to be used as an explicit Future[http://en.wikipedia.org/wiki/Futures_and_promises]. However, this means the current thread may stall for an indefinite amount of time.  Alternatively, both +join+ and +value+ can be called with a block, which will be invoked asynchronously when the job has completed:

	job.join { puts @sum + @product }

	job.value { |v| puts v }


=====  Variable Scope

The asynchronous blocks used by GCD are (almost) just standard ruby blocks, and thus have access to the local context:

	filename = "/etc/passwd"
	Dispatch.fork { File.open(filename) {|f| puts f} }

The one caveat is that since the block may run at a later time, local (dynamic) variables are always copied rather than referenced:

	filename = "/etc/passwd"
	1.times do { filename = "/etc/group" }
	p filename # => "/etc/group"
	job = Dispatch.fork { filename = "/etc/shell" }
	job.join { p filename } # => "/etc/group"

In practice this is not a significant limitation, since it only copies the _variable_ -- not the object itself. Thus, operations that mutate (i.e., modify in place) the underlying object -- unlike those that reassign the variable -- behave as expected:

	ary = ["/etc/passwd"]
	job = Dispatch.fork { ary << "/etc/shell" }
	job.join { p ary } # => ["/etc/passwd", "/etc/shell"]

In this case, the local variable +ary+ still points to the same object that was mutated, so it contains the new value.

Note however that Ruby treats the accumulation operations ("+=", "||=", etc.) as syntactic sugar over assignment, and thus those operations only affect the copy of the variable:

	ary = ["/etc/passwd"]
	Dispatch.fork { ary += ["/etc/shell"] }
	job.join { p ary } # => ["/etc/passwd" ]

When in doubt, simply use instance or global variables (e.g., anything with a sigil[http://en.wikipedia.org/wiki/Sigil_(computer_programming)]), as those have a well-defined existence outside the local scope, and are thus referenced directly by the dispatched block:

	@ary = ["/etc/passwd"]
	Dispatch.fork { @ary += ["/etc/shell"] }
	job.join { p @ary } # => ["/etc/passwd", "/etc/shell"]


== Synchronization

Those of you who have done multithreaded programming in other languages may have (rightly!) shuddered at the early mention of mutating an object asynchronously, as that could lead to data corruption when done from multiple threads:

	s = "/etc/passwd"
	# NEVER do this
	Dispatch.async { s.gsub!("passwd", "shell") }
	Dispatch.async { s.gsub!("passwd", "group") }
	s # => who knows? "/etc/shoup"?

Because Ruby 1.8 had a global VM lock (or GIL[http://en.wikipedia.org/wiki/Global_Interpreter_Lock]), you never had to worry about issues like these, as everything was automatically serialized by the interpreter. True, this reduced the benefit of using multiple threads, but it also meant that Ruby developers didn't have to learn about locks[http://en.wikipedia.org/wiki/Lock_(computer_science)], mutexes[http://en.wikipedia.org/wiki/Mutual%20eclusion], deadlock[http://en.wikipedia.org/wiki/Deadlock], and {priority inversion}[http://en.wikipedia.org/wiki/Priority_inversion].


=== Dispatch::Queue

Fortunately, even though MacRuby no longer has a global VM lock, you (mostly) still don't need to know about all those things, because GCD provides {lock-free synchronization}[http://en.wikipedia.org/wiki/Non-blocking_synchronization] via serial queues. In contrast to the _concurrent_ queues GCD automatically creates for every process ("global"), the queues you create yourself ("private") are _serial_ -- that is, they only run one block at a time. 

==== Dispatch.queue

Instead of locks, simply create a serial queue for each object you need to protect using the +queue+ method:

	s = "/etc/passwd"
	q_s = Dispatch.queue(s)
	# => Dispatch.comparable.nsstring.nsmutablestring.8592077600.1266360156.88218
	q_s.class # => Dispatch::Queue
	
That long funny-looking string is the queue's +label+, a (relatively) unique combination of the passed object's inheritance chain,  id, and the queue creation time.  It is useful for debugging, as it is displayed in log messages and the {Dispatch  Instrument}[http://developer.apple.com/iPhone/library/documentation/DeveloperTools/Conceptual/InstrumentsUserGuide/Built-InInstruments/Built-InInstruments.html#//apple_ref/doc/uid/TP40004652-CH6-SW41].

==== Queue#async

To schedule a block on a serial queue, just call +async+::

	q_s.async { s.gsub!("passwd", "shell") }
	q_s.async { s.gsub!("passwd", "group") }
	
This is analogous as +Dispatch.async(priority)+, which is actually a convenience function wrapping:
	
	Dispatch::Queue.concurrent(priority).async

where +Dispatch::Queue.concurrent(priority)+ returns the global concurrent queue for a given +priority+.

===== Implementation Note

For the curious, here's what happens behind the scenes.  When you add a block to an empty serial queue, GCD in turn adds that queue to the default concurrent queue, just as if it were a block. 

==== Queue#sync

But wait, how do we know when that work has been completed? By calling +async+'s synchronous cousin, +sync+:

	q.sync { p s } # => "/etc/shell", since that always runs first

Because serial queues process each block in the order it was received, calling +sync+ after a series of +async+ calls ensures that everything has completed. It is important to only call +sync+ when absolutely necessary, though, as it stops the current thread and thus increases the risk of deadlock, priority inversion, and all the other scary things I said you didn't need to worry about.

==== Group#new

Of course, using a serial queue alone isn't very interesting, as it doesn't provide any concurrency.  The real power comes from using concurrent queues to do the computationally-intensive work, and serial queues to safely save the results to a shared data structure. The easiest way to do this is by creating and passing an explicit +Group+ object, which can be used as a parameter to +Queue#async+:

	@sum = 0
	q_sum = Dispatch.queue(@sum)
	g = Dispatch::Group.new
	Dispatch.async(g) do
		v1 = Math.sqrt(10**100)
		q_sum.async(g) { @sum += v1 }
	end
	Dispatch.async(g) do
		v2 = Math.sqrt(10**1000)
		q_sum.async(g) { @sum += v2 }
	end

	g.join { p @sum }

The invocations nest, so Group#join won't return until +q_sum.async(g)+ has completed, even if it hadn't yet been submitted.

=== Dispatch::Actor

This pattern of associating a queue with an object is so common and powerful it even has a name: the {Actor model}[http://en.wikipedia.org/wiki/Actor_model].  MacRuby provides a +Dispatch::Actor+ class that uses SimpleDelegator[http://ruby-doc.org/stdlib/libdoc/delegate/rdoc/index.html] to guard access to any object, so that all invocations automatically occur on a private serial queue.
	
==== Dispatch.wrap

The simplest way to turn an object into an actor is with the +wrap+ method:

	s = "/etc/passwd"
	a_s = Dispatch.wrap(s)

That's it! Apart from a small number of basic methods (e.g., +class+, +object_id+), everything is passed to the delegated object for execution:

	a_s.gsub!("passwd", "shell")
	p a_s.to_s # => "/etc/shell"
	
By default, wrapped methods are invoked synchronously and immediately return their value:
	
	a_s.gsub("shell", "group") # => "/etc/group"

But like everything else in GCD, we'd rather you pass it a block so it can execute asynchronously:

	a_s.gsub("group", "passwd") {|v| p v }# => "/etc/passwd"

Voila, make any object thread-safe, asynchronous, and concurrent using a single line of Ruby!


	
	
=== Actor#__with__(group)
	
All invocations on the internal serial queue occur asynchronously. To keep track of them, pass it a group using the intrinsic (non-delegated) method +__with__+:

	g = Dispatch::Group.new
	ary.__with__(g)
	Dispatch.async(g) { ary << Math.sqrt(10**100) }
	Dispatch.async(g) { ary << Math.sqrt(10**1000) }
	g.join { p ary.to_s }



	puts "\n Use Dispatch.wrap to serialize object using an Actor"
	b = Dispatch.wrap(Array)
	b << "safely change me"
	p b.size # => 1 (synchronous return)
	b.size {|n| p "Size=#{n}"} # => "Size=1" (asynchronous return)


== Iteration

You use the default queue to run a single item in the background or to run many operations at once.  For the common case of a “parallel for loop”,  GCD provides an optimized “apply” function that submits a block for each iteration:

	#define COUNT 128
	__block double result[COUNT];
	dispatch_apply(COUNT, q_default, ^(size_t i){
	 	result[i] = complex_calculation(i);
	 });
	double sum = 0;
	for (int i=0; i < COUNT; i++) sum += result[i];


== Events

In addition to scheduling blocks directly, developers can set a block as the handler for event sources such as:

* Timers
* Signals
* File descriptors and sockets
* Process state changes
* Mach ports
* Custom application-specific events
	
When the source “fires,” GCD will schedule the handler on the specific queue if it is not currently running, or coalesce pending events if it is. This provides excellent responsiveness without the expense of either polling or binding a thread to the event source.  Plus, since the handler is never run more than once at a time, the block doesn’t even need to be reentrant.

=== Timer Example

For example, this is how you would create a timer that prints out the current time every 30 seconds -- plus 5 microseconds leeway, in case the system wants to align it with other events to minimize power consumption.


	dispatch_source_t timer = dispatch_source_create(DISPATCH_SOURCE_TYPE_TIMER, 0, 0, q_default); //run event handler on the default global queue
	dispatch_time_t now = dispatch_walltime(DISPATCH_TIME_NOW, 0);
	dispatch_source_set_timer(timer, now, 30ull*NSEC_PER_SEC, 5000ull);
	dispatch_source_set_event_handler(timer, ^{
		printf(“%s\n”, ctime(time(NULL)));
	});

Sources are always created in a suspended state to allow configuration, so when you are all set they must be explicitly resumed to begin processing events. 

	dispatch_resume(timer);

You can suspend a source or dispatch queue at any time to prevent it from executing new blocks, though this will not affect blocks that are already being processed.


=== Custom Events Example

GCD provides two different types of user events, which differ in how they coalesce the data passed to dispatch_source_merge_data:

DISPATCH_SOURCE_TYPE_DATA_ADD: accumulates the sum of the event data (e.g., for numbers)
DISPATCH_SOURCE_TYPE_DATA_OR: combines events using a logical OR (e.g, for booleans or bitmasks)

Though it is arguably overkill, we can even use events to rewrite our dispatch_apply example. Since the event handler is only ever called once at a time, we get automatic serialization over the "sum" variable without needing to worry about reentrancy or private queues:

	__block unsigned long sum = 0;
	dispatch_source_t adder = dispatch_source_create(DISPATCH_SOURCE_TYPE_DATA_ADD, 0, 0, q_default);
	dispatch_source_set_event_handler(adder, ^{
		sum += dispatch_source_get_data(adder);
	});
	dispatch_resume(adder);

	#define COUNT 128
	dispatch_apply(COUNT, q_default, ^(size_t i){
		unsigned long x = integer_calculation(i);
		dispatch_source_merge_data(adder, x);
	});
	dispatch_release(adder);

Note that for this example we changed our calculation to use integers, as dispatch_source_merge_data expects an unsigned long parameter.  

=== File Descriptor Example

Here is a more sophisticated example involving reading from a file. Note the use of non-blocking I/O to avoid stalling a thread:

	int fd = open(filename, O_RDONLY);
	fcntl(fd, F_SETFL, O_NONBLOCK);  // Avoid blocking the read operation
	dispatch_source_t reader = 
	  dispatch_source_create(DISPATCH_SOURCE_TYPE_READ, fd, 0, q_default); 

  
We will also specify a “cancel handler” to clean up our descriptor:

	dispatch_source_set_cancel_handler(reader, ^{ close(fd); } );

The cancellation will be invoked from the event handler on, e.g., end of file:

	typedef struct my_work {…} my_work_t;
	dispatch_source_set_event_handler(reader, ^{ 
		size_t estimate = dispatch_source_get_data(reader);
		my_work_t *work = produce_work_from_input(fd, estimate);
		if (NULL == work)
			dispatch_source_cancel(reader);
		else
			dispatch_async(q_default, ^{ consume_work(work); free(work); } );
	});
	dispatch_resume(reader);

To avoid bogging down the reads, the event handler packages up the data in a my_work_t and schedules the processing in another block.  This separation of  concerns is known as the producer/consumer pattern, and maps very naturally to Grand Central Dispatch queues.  In case of imbalance, you may need to adjust the relative priorities of the producer and consumer queues or throttle them using semaphores.

== Semaphores 

Finally, GCD has an efficient, general-purpose signaling mechanism known as dispatch semaphores.  These are most commonly used to throttle usage of scarce resources, but can also help track completed work:  


	dispatch_semaphore_t sema = dispatch_semaphore_create(0);
	dispatch_async(a_queue, ^{ some_work(); dispatch_semaphore_signal(sema); });
	more_work(); 
	dispatch_semaphore_wait(sema, DISPATCH_TIME_FOREVER);
	dispatch_release(sema);
	do_this_when_all_done();

Like other GCD objects, dispatch semaphores usually don’t need to call into the kernel, making them much faster than regular semaphores when there is no need to wait.

= Conclusion

Grand Central Dispatch is a new approach to building software for multicore systems, one in which the operating system takes responsibility for the kinds of thread management tasks that traditionally have been the job of application developers. Because it is built into Mac OS X at the most fundamental level, GCD can not only simplify how developers build their code to take advantage of multicore, but also deliver better performance and efficiency than traditional approaches such as threads.  With GCD, Snow Leopard delivers a new foundation on which Apple and third party developers can innovate and realize the enormous power of both today’s hardware and tomorrow’s. 

