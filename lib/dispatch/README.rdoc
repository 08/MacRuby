= Grand Central Dispatch for MacRuby

== Introduction

This article explains how to use Grand Central Dispatch (*GCD*) from MacRuby, and is adapted from {Introducing Blocks and Grand Central Dispatch}[http://developer.apple.com/mac/articles/cocoa/introblocksgcd.html] at the {Apple Developer Connection}[http://developer.apple.com/].

=== About GCD
	
GCD is a revolutionary approach to multicore computing that is woven throughout the fabric of {Mac OS X}[http://www.apple.com/macosx/] version 10.6 Snow Leopard. GCD combines an easy-to-use programming model with highly-efficient system services to radically simplify the code needed to make best use of multiple processors. The technologies in GCD improve the performance, efficiency, and responsiveness of Snow Leopard out of the box, and will deliver even greater benefits as more developers adopt them.

The central insight of GCD is shifting the responsibility for managing threads and their execution from applications to the operating system. As a result, programmers can write less code to deal with concurrent operations in their applications, and the system can perform more efficiently on single-processor machines, large multiprocessor servers, and everything in between. Without a pervasive approach such as GCD, even the best-written application cannot deliver optimal performance, because it doesn't have full insight into everything else happening in the system. 

=== The MacRuby Dispatch module

GCD is natively implemented as a C API and runtime engine.  MacRuby 0.5 introduces a new "Dispatch" module, which provides a Ruby wrapper around that API. This allows Ruby blocks to be scheduled on queues for asynchronous and concurrent execution either explicitly or in response to various kinds of events, with GCD automatically mapping queues to threads as needed.  The Dispatch module provides four primary abstractions that mirror the C API:

Dispatch::Queue:: The basic unit for organizing blocks. Several queues are created by default, and applications may create additional queues for their own use.

Dispatch::Group:: Allows applications to track the progress of blocks submitted to queues and take action when the blocks complete.

Dispatch::Source:: Monitors and coalesces low-level system events so that they can be responded to asynchronously via simple event handlers.

Dispatch::Semaphore:: Synchronizes threads via a combination of waiting and signalling.

In addition, MacRuby 0.6 provides additional, higher-level abstractions and convenience APIs such as +Job+ and +Proxy+ via the "dispatch" library (i.e., +require 'dispatch'+).

=== What You Need

As the MacRuby 0.6 features help reduce the learning curve for GCD, we will assume those for the remainder of this article.  Note that MacRuby 0.6 is currently (as of Feb 2010) only available via the source[http://www.macruby.org/source.html] or the {nightly builds}[http://www.icoretech.org/2009/09/macruby-nightlies/].

We also assume that you are already familiar with Ruby, though not necessarily MacRuby. No prior knowledge of C or GCD is assumed or required, but the {dispatch(3) man page}[http://developer.apple.com/mac/library/DOCUMENTATION/Darwin/Reference/ManPages/man3/dispatch.3.html] may be helpful if you wish to better understand the underlying semantics.

== Dispatch::Job: Easy Concurrency

The easiest way to perform concurrent work is via a +Job+ object. Say you have a complex, long-running calculation you want to happen in the background. Create a job by passing the block you want to execute:

	require 'dispatch'
	job = Dispatch::Job.new { Math.sqrt(10**100) }

This atomically[http://en.wikipedia.org/wiki/Atomic_operation] adds the block to GCD's default concurrent queue, then returns immediately so you don't stall the main thread.

Concurrent queues schedule as many simultaneous blocks as they can on a first-in/first-out (FIFO[http://en.wikipedia.org/wiki/FIFO]) basis, as long as there are threads available.  If there are spare CPUs, the system will automatically create more threads -- and reclaim them when idle -- allowing GCD to dynamically scale the number of threads based on the overall system load.  Thus (unlike with threads, which choke when you create too many) you can generally create as many jobs as you want, and GCD will do the right thing. 

=== Job#value: Asynchronous Return Values

The downside of asynchrony is that you don't know exactly when your job will execute.  Fortunately, +Dispatch::Job+ duck-types +Thread[http://ruby-doc.org/core/classes/Thread.html]+ as much as possible, so you can call +value[http://ruby-doc.org/core/classes/Thread.html#M000460]+ to obtain the result of executing that block:

	@result = job.value
	puts @result.to_int.to_s.size # => 51
	
This will wait until the value has been calculated, allowing it to be used as an {explicit Future}[http://en.wikipedia.org/wiki/Futures_and_promises]. However, this may stall the main thread indefinitely, which reduces the benefits of concurrency.  

Wherever possible, you should instead attempt to figure out exactly _when_  and _why_ you need to know the result of asynchronous work. Then, call +value+ with a block to also perform _that_ work asynchronously once the value has been calculated -- all without blocking the main thread:

	job.value {|v| p v.to_int.to_s.size } # => 51 (eventually)

=== Job#join: Job Completion

If you just want to track completion, you can call +join[http://ruby-doc.org/core/classes/Thread.html#M000462]+, which waits without returning the result:

	job.join
	puts "All Done"
	
Similarly, call +join+ with a block to run asynchronously once the work has been completed

	job.join { puts "All Done" }

=== Job#add: Coordinating Work

More commonly, you will have multiple units of work you'd like to perform in parallel.  You can add blocks to an existing job using +add:

	job.add { Math.sqrt(2**64) }

If there are multiple blocks in a job, +value+ will wait until they all finish then return the last value received:

job.value {|b| p b } # => 4294967296.0

=== Job#values: Returning All Values

Note that values may be received out of order, since they may take differing amounts of time to complete. If you need to force a particular ordering, create a new +Job+ or call +join+ before submitting the block.

Additionally, you can call +values+ to obtain all the values:

	@values = job.values 
	puts @values # => [1.0E50, 4294967296.0]

Note that unlike +value+ this will not +wait+ or +join+, and thus does not have an asynchronous equivalent.

== Dispatch::Proxy: Protecting Shared Data

Concurrency would be easy if everything was {embarrassingly parallel}[http://en.wikipedia.org/wiki/Embarrassingly_parallel], but it becomes tricky when we need to share data between threads. If two threads try to modify the same object at the same time, it could lead to inconsistent (read: _corrupt_) data.  There are well-known techniques for preventing this sort of data corruption (e.g. locks[http://en.wikipedia.org/wiki/Lock_(computer_science)] andmutexes[http://en.wikipedia.org/wiki/Mutual%20eclusion]), but these have their own well-known problems (e.g., deadlock[http://en.wikipedia.org/wiki/Deadlock], and {priority inversion}[http://en.wikipedia.org/wiki/Priority_inversion]).

Because Ruby traditionally had a global VM lock (or GIL[http://en.wikipedia.org/wiki/Global_Interpreter_Lock]), only one thread could modify data at a time, so developers never had to worry about these issues; then again, this also meant they didn't get much benefit from additional threads.  

In MacRuby, every thread has its own Virtual Machine, which means all of them can access Ruby objects at the same time -- great for concurrency, not so great for data integrity. Fortunately, GCD provides _serial queues_ for {lock-free synchronization}[http://en.wikipedia.org/wiki/Non-blocking_synchronization], by ensuring that only one thread a time accesses a particular object -- without the complexity and inefficiency of locking. Here we will focus on +Dispatch::Proxy+, a high-level construct that implements the {Actor model}[http://en.wikipedia.org/wiki/Actor_model] by wrapping any arbitrary Ruby object with a +SimpleDelegate+ that only allows execution of one method at a time (i.e., serializes data access on to a private queue).

=== Job#synchronize: Creating Proxies

The easiest way to create a Proxy is to first create an empty Job:

	job = Dispatch::Job.new

then ask it to wrap the object you want to modify from multiple threads:

	@hash = job.synchronize {}
	puts @hash.class # => Dispatch::Proxy
	
This is actually the same type of object used to manage the list of +values+:

	puts job.values.class # => Dispatch::Proxy
	
=== method_missing: Using Proxies

The Proxy object can be called just as it if were the delegate object:

	@hash[:foo] = :bar
	puts @hash.to_s  # => "{:foo=>:bar}"
	
Except that you can use it safely inside Dispatch blocks from multiple threads: 
	
	[64, 100].each do |n|
		job << { @hash[n] = Math.sqrt(10**n) }
	end
	puts @hash.inspect # => {64 => 1.0E32, 100 => 1.0E50}

In this case, each block will perform the +sqrt+ asynchronously on the concurrent queue, potentially on multiple threads
	
As with Dispatch::Job, you can make any invocation asynchronous by passing a block:

	@hash.inspect { |s| p s } # => {64 => 1.0E32, 100 => 1.0E50}

=== \_\_value\_\_: Returning Delegate

If for any reason you need to retrieve the delegate object, simply call +__value__+:

	delegate = @hash.__value__
	puts delegate.class # => Hash
	
This differs from +SimpleDelegate#__getobj__+ in it will first wait until any pending asynchronous blocks have executed.

As elsewhere in Ruby, the "__" namespace implies "internal" methods, in this case meaning they are called directly on the proxy rather than passed to the delegate. 

====  Caveat: Local Variables

Because Dispatch blocks may execute after the local context has gone away, you should always store Proxy objects in a non-local  variable: instance, class, or global -- anything with a sigil[http://en.wikipedia.org/wiki/Sigil_(computer_programming)]. 

Note that we can as usual _access_ local variables from inside the block; GCD automatically copies them, which is why this works as expected:

	n = 42
	job = Dispatch::Job.new { p n }
	job.join # => 42
	
but this doesn't:

	n = 0
	job = Dispatch::Job.new { n = 42 }
	job.join
	p n # => 0 

The general rule is to "do *not* assign to external variables inside a Dispatch block."  Assigning local variables will have no effect (outside that block), and assigning other variables may replace your Proxy object with a non-Proxy version.  Remember also that Ruby treats the accumulation operations ("+=", "||=", etc.) as syntactic sugar over assignment, and thus those operations only affect the copy of the variable:

	n = 0
	job = Dispatch::Job.new { n += 42 }
	job.join
	p n # => 0 

== Dispatch Enumerable: Parallel Iterations

Jobs are useful when you want to run a single item in the background or to run many different operations at once. But if you want to run the _same_ operation multiple times, you can take advantage of specialized GCD iterators.  The Dispatch module defines "p_" variants of common Ruby iterators, making it trivial to parellelize existing operations.  

In addition, for simplicity they all are _synchronous_, meaning they won't return until all the work has completed.

=== Integer#p_times

The simplest iteration is defined on the +Integer+ class, and passes the index that many +times+:

	5.p_times { |i| puts 10**i } # => 1  100 1000 10 10000 
	
Note that even though the iterator as a whole is synchronous, and blocks are scheduled in the order received, each block runs independently and  therefore may complete out of order.

This does add some overhead compared to the non-parallel version, so if you have a large number of relatively cheap iterations you can batch them together by specifying a +stride+:

	5.p_times(3) { |i| puts 10**i } # =>1000 10000 1 10 100 

It doesn't change the result, but schedules fewer blocks thus amortizing the overhead over more work. Note that items _within_ a stride are executed in the original order, but no order is guaranteed _between_ strides.

The +p_times+ method is used to implement several convenience methods on +Enumerable+, which are therefore available from any class which mixes that in (e.g, +Array+, +Hash+, etc.). These also can take an optional stride.

=== Enumerable#p_each

Passes each object, like +each+:

	%w(Mon Tue Wed Thu Fri).p_each { |day| puts day} # => Mon Wed Thu Tue Fri

	%w(Mon Tue Wed Thu Fri).p_each(3) { |day| puts day} # =>  Thu Fri Mon Tue Wed

=== Enumerable#p_each_with_index

Passes each object and its index, like +each_with_index+:

	%w(Mon Tue Wed Thu Fri).p_each { |day, i | puts "#{i}:#{day}"} # => 0:Mon 2:Wed 3:Thu 1:Tue 4:Fri

	%w(Mon Tue Wed Thu Fri).p_each(3) { |day, i | puts "#{i}:#{day}"} # => 3:Thu 4:Fri 0:Mon 1:Tue 2:Wed 

=== Enumerable#p_map

Passes each object and collects the transformed values, like +map+:

	(0..4).p_map { |i| 10**i } # => [1, 1000, 10, 100, 10000]

	(0..4).p_map(3) { |i| 10**i } # => [1000, 10000, 1, 10, 100]

=== Enumerable#p_mapreduce

Unlike the others, this method does not have a serial equivalent, but you may recognize it from the world of {distributed computing}[http://en.wikipedia.org/wiki/MapReduce]:

	(0..4).p_mapreduce(0) { |i| 10**i } # => 11111

This uses a parallel +inject+ (formerly known as +reduce+) to return a single value by combining the result of +map+. 	Unlike +inject+, you must specify an explicit initial value as the first parameter. The default accumulator is ":+", but you can specify a different symbol to +send+:

	(0..4).p_mapreduce([], :concat) { |i| [10**i] } # => [1, 1000, 10, 100, 10000]
	
Because of those parameters, the optional +stride+ is now the third:

	(0..4).p_mapreduce([], :concat, 3) { |i| [10**i] } # => [1000, 10000, 1, 10, 100]

=== Enumerable#p_findall

Passes each object and collects those for which the block is true, like +findall+:

	(0..4).p_findall { |i| i.odd?} # => {3, 1}

	(0..4).p_findall(3) { |i| i.odd?} # => {3, 1}
	
=== Enumerable#p_find

Passes each object and returns nil if none match. Similar to +find+, it returns the first object it _finds_ for which the block is true, but unlike +find+ that may not be the _actual_ first object since blocks -- say it with me -- "may complete out of order":

	(0..4).p_findall { |i| i == 5 } # => nil

	(0..4).p_findall { |i| i.odd?} # => 1

	(0..4).p_findall(3) { |i| i.odd?} # => 3

== Dispatch::Sources: Asynchronous Events

In addition to scheduling blocks directly, GCD makes it easy to run a block in response to various system events via a Dispatch::Source, which supports:

* Timers
* Signals
* File descriptors and sockets
* Process state changes
* Mach ports
* Custom application-specific events
	
When the source “fires,” GCD will schedule the handler on the specific queue if it is not currently running, or -- more importantly -- coalesce pending events if it is. This provides excellent responsiveness without the expense of either polling or binding a thread to the event source.  Plus, since the handler is never run more than once at a time, the block doesn’t even need to be reentrant.

---
= UNDER CONSTRUCTION = 
=== Timer Example

For example, this is how you would create a timer that prints out the current time every 30 seconds -- plus 5 microseconds leeway, in case the system wants to align it with other events to minimize power consumption.


	dispatch_source_t timer = dispatch_source_create(DISPATCH_SOURCE_TYPE_TIMER, 0, 0, q_default); //run event handler on the default global queue
	dispatch_time_t now = dispatch_walltime(DISPATCH_TIME_NOW, 0);
	dispatch_source_set_timer(timer, now, 30ull*NSEC_PER_SEC, 5000ull);
	dispatch_source_set_event_handler(timer, ^{
		printf(“%s\n”, ctime(time(NULL)));
	});

Sources are always created in a suspended state to allow configuration, so when you are all set they must be explicitly resumed to begin processing events. 

	dispatch_resume(timer);

You can suspend a source or dispatch queue at any time to prevent it from executing new blocks, though this will not affect blocks that are already being processed.


=== Custom Events Example

GCD provides two different types of user events, which differ in how they coalesce the data passed to dispatch_source_merge_data:

DISPATCH_SOURCE_TYPE_DATA_ADD: accumulates the sum of the event data (e.g., for numbers)
DISPATCH_SOURCE_TYPE_DATA_OR: combines events using a logical OR (e.g, for booleans or bitmasks)

Though it is arguably overkill, we can even use events to rewrite our dispatch_apply example. Since the event handler is only ever called once at a time, we get automatic serialization over the "sum" variable without needing to worry about reentrancy or private queues:

	__block unsigned long sum = 0;
	dispatch_source_t adder = dispatch_source_create(DISPATCH_SOURCE_TYPE_DATA_ADD, 0, 0, q_default);
	dispatch_source_set_event_handler(adder, ^{
		sum += dispatch_source_get_data(adder);
	});
	dispatch_resume(adder);

	#define COUNT 128
	dispatch_apply(COUNT, q_default, ^(size_t i){
		unsigned long x = integer_calculation(i);
		dispatch_source_merge_data(adder, x);
	});
	dispatch_release(adder);

Note that for this example we changed our calculation to use integers, as dispatch_source_merge_data expects an unsigned long parameter.  

=== File Descriptor Example

Here is a more sophisticated example involving reading from a file. Note the use of non-blocking I/O to avoid stalling a thread:

	int fd = open(filename, O_RDONLY);
	fcntl(fd, F_SETFL, O_NONBLOCK);  // Avoid blocking the read operation
	dispatch_source_t reader = 
	  dispatch_source_create(DISPATCH_SOURCE_TYPE_READ, fd, 0, q_default); 

  
We will also specify a “cancel handler” to clean up our descriptor:

	dispatch_source_set_cancel_handler(reader, ^{ close(fd); } );

The cancellation will be invoked from the event handler on, e.g., end of file:

	typedef struct my_work {…} my_work_t;
	dispatch_source_set_event_handler(reader, ^{ 
		size_t estimate = dispatch_source_get_data(reader);
		my_work_t *work = produce_work_from_input(fd, estimate);
		if (NULL == work)
			dispatch_source_cancel(reader);
		else
			dispatch_async(q_default, ^{ consume_work(work); free(work); } );
	});
	dispatch_resume(reader);

To avoid bogging down the reads, the event handler packages up the data in a my_work_t and schedules the processing in another block.  This separation of  concerns is known as the producer/consumer pattern, and maps very naturally to Grand Central Dispatch queues.  In case of imbalance, you may need to adjust the relative priorities of the producer and consumer queues or throttle them using semaphores.

== Semaphores 

Finally, GCD has an efficient, general-purpose signaling mechanism known as dispatch semaphores.  These are most commonly used to throttle usage of scarce resources, but can also help track completed work:  


	dispatch_semaphore_t sema = dispatch_semaphore_create(0);
	dispatch_async(a_queue, ^{ some_work(); dispatch_semaphore_signal(sema); });
	more_work(); 
	dispatch_semaphore_wait(sema, DISPATCH_TIME_FOREVER);
	dispatch_release(sema);
	do_this_when_all_done();

Like other GCD objects, dispatch semaphores usually don’t need to call into the kernel, making them much faster than regular semaphores when there is no need to wait.

= Conclusion

Grand Central Dispatch is a new approach to building software for multicore systems, one in which the operating system takes responsibility for the kinds of thread management tasks that traditionally have been the job of application developers. Because it is built into Mac OS X at the most fundamental level, GCD can not only simplify how developers build their code to take advantage of multicore, but also deliver better performance and efficiency than traditional approaches such as threads.  With GCD, Snow Leopard delivers a new foundation on which Apple and third party developers can innovate and realize the enormous power of both today’s hardware and tomorrow’s. 

